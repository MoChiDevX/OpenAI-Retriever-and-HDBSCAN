# ä¸»åº”ç”¨å…¥å£ï¼Œç”¨äºåŠ è½½ Streamlit é¡µé¢å¹¶è°ƒç”¨å„åŠŸèƒ½æ¨¡å—
import streamlit as st
from clustering.cluster import load_vector_data, cluster_vectors, visualize_clusters
from clustering.utils import parse_vector_str
from embeddings.extrator import extract_text_vectors
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO
import zipfile



# è®¾ç½®é¡µé¢å®½åº¦æ¨¡å¼
st.set_page_config(layout="wide")

# ä¾§è¾¹æ åŠŸèƒ½é€‰æ‹©å™¨
st.sidebar.title("åŠŸèƒ½é€‰æ‹©")
option = st.sidebar.radio("è¯·é€‰æ‹©è¦è¿è¡Œçš„åŠŸèƒ½æ¨¡å—ï¼š", ["æå–æ–‡æœ¬å‘é‡", "HDBSCANèšç±»" ,"å‘é‡å¯è§†åŒ–"])

# èšç±»æ¨¡å—
if option == "HDBSCANèšç±»":
    st.header("ğŸ” HDBSCAN èšç±»åˆ†æ")
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ åŒ…å«å‘é‡çš„ csv æ–‡ä»¶", type=["csv", "parquet"])
    min_cluster_size = st.text_input("æœ€å°èšç±»å¤§å°(å»ºè®®3~10)", value="5")  # ç”¨æˆ·è¾“å…¥æœ€å°èšç±»å°ºå¯¸
    submit2 = st.button('åŠ è½½èšç±»')
    if uploaded_file is not None and submit2:
        with st.spinner('AI æ­£åœ¨åŠ è½½å¹¶å¤„ç†èšç±»ï¼Œè¯·ç¨å€™...'):
            df = load_vector_data(uploaded_file)
            df_clustered, model = cluster_vectors(df, int(min_cluster_size))
            vectors = np.array(df['vector'].tolist())
            similarity = cosine_similarity(vectors[:100])
            st.write(f"ğŸ”¥ å‰100æ¡å‘é‡å¹³å‡ç›¸ä¼¼åº¦ï¼š{similarity.mean():.4f}\né«˜ç›¸ä¼¼åº¦ï¼ˆ>0.8ï¼‰ï¼šè®¾ç½®è¾ƒå°çš„ min_cluster_sizeï¼ˆä¾‹å¦‚ 5ï¼‰ï¼Œè¿™æ ·èšç±»ä¼šå°†ç›¸ä¼¼çš„ç‚¹åˆå¹¶åˆ°å°‘æ•°ç±»åˆ«ä¸­ã€‚\nä¸­ç­‰ç›¸ä¼¼åº¦ï¼ˆ0.5 â‰¤ avg_similarity < 0.8ï¼‰ï¼šè®¾ç½®é€‚ä¸­çš„ min_cluster_sizeï¼ˆä¾‹å¦‚ 10ï¼‰ï¼Œèšç±»ç»“æœä¼šæœ‰é€‚ä¸­çš„ç±»åˆ«æ•°ã€‚\nä½ç›¸ä¼¼åº¦ï¼ˆ<0.5ï¼‰ï¼šè®¾ç½®è¾ƒå¤§çš„ min_cluster_sizeï¼ˆä¾‹å¦‚ 20ï¼‰ï¼Œè¿™æ ·èšç±»ä¼šäº§ç”Ÿæ›´å¤šçš„ç±»åˆ«ï¼Œå› ä¸ºæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼åº¦è¾ƒä½ã€‚")
            st.success("âœ… èšç±»å®Œæˆï¼Œ-1ç±» ä»£è¡¨ å™ªå£°")
            st.dataframe(df_clustered)
            st.download_button("ä¸‹è½½èšç±»ç»“æœä¸º CSV", df_clustered.to_csv(index=False), file_name="clustered_result.csv")

# æ–‡æœ¬å‘é‡ç”Ÿæˆæ¨¡å—
elif option == "æå–æ–‡æœ¬å‘é‡":
    st.header("ğŸ“„ æ–‡æœ¬å‘é‡ç”Ÿæˆ")
    st.write('###### æ³¨ï¼šéœ€ç§‘å­¦ä¸Šç½‘')
    openai_key = st.text_input("ğŸ”‘è¯·è¾“å…¥ä½ çš„ OpenAI API Keyï¼š", type="password")
    st.markdown('[OpenAI Keyè·å–æ–¹å¼](https://platform.openai.com/api-keys)')
    uploaded_txt = st.file_uploader("ğŸ“„ ä¸Šä¼  .txt æ–‡æœ¬æ–‡ä»¶", type="txt")

    # ç”¨æˆ·é€‰æ‹©OpenAIåµŒå…¥æ¨¡å‹
    openai_model = st.selectbox(
        "è¯·é€‰æ‹©OpenAIåµŒå…¥æ¨¡å‹ï¼š",
        [
            "text-embedding-ada-002(æ¨èæ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”æˆæœ¬ä½)",
            "text-embedding-babbage-001(é€‚ç”¨äºç¨å¤æ‚çš„ä»»åŠ¡)",
            "text-embedding-curie-001(æ›´é«˜è´¨é‡ä½†æ›´é«˜æˆæœ¬çš„æ¨¡å‹)"
        ]
    )

    # ç”¨æˆ·é€‰æ‹© chunk_size å’Œ chunk_overlap
    chunk_size = st.number_input("è¾“å…¥æ¯æ®µçš„æœ€å¤§å­—ç¬¦æ•° (chunk_size)", min_value=100, max_value=1000, value=300, step=50)
    chunk_overlap = st.number_input("è¾“å…¥æ®µè½çš„é‡å å­—ç¬¦æ•° (chunk_overlap)", min_value=10, max_value=500, value=50, step=10)

    submit1 = st.button('æå–æ–‡æœ¬å‘é‡')
    if uploaded_txt and openai_key and submit1:
        with st.spinner("AI æ­£åœ¨æå–æ–‡æœ¬å‘é‡..."):
            # ä¼ é€’æ‰€é€‰çš„æ¨¡å‹åå’Œç”¨æˆ·è¾“å…¥çš„ chunk_size ä¸ chunk_overlap
            df, name_base = extract_text_vectors(uploaded_txt, openai_key, openai_model, chunk_size, chunk_overlap)

            # ä¸‹è½½ CSV æ–‡ä»¶
            st.download_button("ä¸‹è½½æ–‡æœ¬å‘é‡ä¸º CSV", df.to_csv(index=False), file_name=f'{name_base}.csv')





elif option == "å‘é‡å¯è§†åŒ–":
    st.header("ğŸ“Š å‘é‡èšç±»å¯è§†åŒ–")
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ å¸¦æœ‰èšç±»æ ‡ç­¾å’Œå‘é‡çš„ csv æ–‡ä»¶", type="csv")
    submit3 = st.button('å¯è§†åŒ–åŠ è½½')
    if uploaded_file and submit3:
        with st.spinner('AI æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾åƒ...'):
            df = load_vector_data(uploaded_file)
            # è¯»å–æ–‡ä»¶åæ£€æŸ¥æ•°æ®
            df['vector'] = df['vector'].apply(parse_vector_str)
            df = df[df['vector'].apply(len) > 0]  # è¿‡æ»¤æ‰ç©ºçš„å‘é‡

            if 'cluster' not in df.columns:
                st.warning("è¯¥æ–‡ä»¶ä¸åŒ…å« 'cluster' åˆ—ï¼Œè¯·å…ˆæ‰§è¡Œèšç±»ä»»åŠ¡ã€‚")
            else:
                # è·å–å›¾åƒå¯¹è±¡

                figs = visualize_clusters(df)
                tab_names = ["PCA", "t-SNE", "UMAP"]

                # åˆå§‹åŒ– zip æ–‡ä»¶ç¼“å†²åŒº
                zip_buffer = BytesIO()
                with zipfile.ZipFile(zip_buffer, "w") as zip_file:
                    for i, (fig, name) in enumerate(zip(figs, tab_names), start=1):
                        # å°†å›¾ç‰‡ä¿å­˜åˆ°ä¸´æ—¶ç¼“å†²åŒº
                        img_buf = BytesIO()
                        fig.set_size_inches(6, 4)  # æ§åˆ¶å›¾åƒå¤§å°
                        fig.savefig(img_buf, format="png", dpi=300, bbox_inches='tight')
                        img_buf.seek(0)

                        # æ·»åŠ åˆ° zip åŒ…
                        zip_file.writestr(f"{name}_visualization.png", img_buf.read())

                # å±•ç¤º tab é¡µé¢
                tabs = st.tabs(tab_names)
                for tab, fig, name in zip(tabs, figs, tab_names):
                    with tab:
                        st.pyplot(fig, use_container_width=True)
                        st.info(f"å½“å‰æ˜¾ç¤ºï¼š{name} é™ç»´å›¾")

                # æä¾› ZIP ä¸‹è½½æŒ‰é’®
                zip_buffer.seek(0)
                st.download_button(
                    label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰å¯è§†åŒ–å›¾åƒä¸º ZIP",
                    data=zip_buffer,
                    file_name="cluster_visualizations.zip",
                    mime="application/zip"
                )

